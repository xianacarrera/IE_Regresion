---
title: "Trabajo de Evaluación Continua de Modelos de Regresión"
subtitle: Curso 2021/2022
author: "Xiana Carrera Alonso, Pablo Díaz Viñambres"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```
TODO BORRAR EJEMPLO TABLAS

\begin{center}
\begin{tabular}{ |c c c| }
 \hline
 celda1 & celda2 & celda3 \\
 \hline
 celda4 & celda5 & celda6 \\  
 celda7 & celda8 & celda9 \\   
  \hline
\end{tabular}
\end{center}

# Introducción
En este documento se describe y documenta el ajuste y análisis de un modelo de regresión lineal simple en base a los datos proporcionados para una variable explicativa X y una variable respuesta Y.  
 
El estudio se fundamentará en los conceptos teóricos relacionados con los modelos de regresión lineal simple y su validación que fueron estudiados a lo largo de los Temas 6 y 7 de la asignatura de Inferencia Estadística. Se hará referencia explícita a los mismos a medida que sean empleados. 
 
Asimismo, se utilizará R para realizar las operaciones necesarias para el análisis. Los detalles relativos al empleo de sus funciones se detallarán o bien en el propio informe o bien a través de comentarios sobre el código. 

# Modelo de regresión lineal simple
Recordemos que un modelo de regresión sirve para representar la dependencia de una variable Y respecto de una o varias variables X. En particular, en el modelo de regresión lineal simple se consideran variables X e Y univariantes (esto es, reflejan el valor de una sola característica) y parte de las hipótesis de linealidad, homocedasticidad y normalidad e independencia de los errores (véase una explicación detallada de las mismas en el ejercicio 7).

Consideraremos una muestra extraída bajo diseño fijo, esto es, con datos $(x_1, Y_1), ..., (x_n, Y_n)$, donde $x_1, ..., x_n$ están fijados por el experimentador.

Así, tendremos
$$
Y_i = \beta_0 + \beta_ 1*x_i + \epsilon_i  \quad \text{para}\; i \in {1,...,n}
$$
donde $\epsilon_1,...,\epsilon_n \in N(0, \sigma^2)$ y son independientes.


Este modelo presenta 3 parámetros: \beta_0
 
# Cuestiones preliminares 
En cada una de las secciones del documento se trabajará con $\textbf{\alpha = 0.01}$ para los distintos contrastes, intervalos de confianza, etc. planteados. Equivalentemente, se empleará un nivel de significación $1 - \alpha = 0.99$. 
```{r alfa}
alfa <- 1 - 0.99; alfa
conf.level <- 1 - alfa; conf.level
```

Esta elección se debe al enunciado del tercer ejercicios, dónde se pide emplear un nivel de significación del 99% para la construcción de intervalos de confianza de los parámetros del modelo. Para mantener entonces la consistencia en todo el informe, se decidió conservarlo en los demás apartados que lo requieren.


## Librerías utilizadas
Cargamos a continuación todas las librerías que utilizaremos a lo largo de la ejecución. Si alguno de los paquetes no ha sido previamente instalado, debe ejecutarse la instrucción \texttt{_install.packages("nombre_del_paquete")}.  

```{r librerias}
# install.packages("paquete_de_ejemplo")
library(ggplot2)      # Para diagrama de dispersión con región de confianza
library(lmtest)
library(sm)
library(rpanel)   
library(viridis)      # Para gradiente de colores en gráfica de normalidad
library(nortest)      # Necesario para lillie.test
library(car)          # Necesario para QQPlot
```

## Lectura de datos
En primer lugar, leemos los datos del archivo proporcionado, que cuenta con 76 variables respuesta, Y1, ..., Y76, y una variable explicativa común, X. En nuestro caso, limitaremos el estudio a Y47, que denotaremos sencillamente como Y de aquí en adelante.

Nada más importar el archivo (para lo cual es necesario que el usuario cambie el directorio actual, empleando, por ejemplo, _setwd_ o _Ctrl + May + H_), realizamos un pequeño análisis estadístico de los datos empleando las funciones estándar _head_, _class_, _names_, _str_ y _summary_.

Por comodidad para cálculos posteriores, también guardamos el número de datos, n.

```{r cars}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # Configurar wd a la carpeta actual (solo en RStudio) 
# Ejemplo de uso de setwd para cambiar el directorio actual:
#setwd("C:\\Users\\Pablo\\Desktop\\IE_Regresion")

# Leemos los datos empleando read.table (por la extensión .txt)
# Indicamos que existe una cabecera, que las columnas están separadas por espacios y que el signo decimal es el punto.
datos <- read.table("datos_trabajo_temas6y7.txt", header=T, sep=" ", dec=".")
# Vemos los nombres de las variables
names(datos)

# Y nos quedamos con las variables de interés
datos <- datos[, c("X", "Y47")]

# Comprobamos la estructura de las primeras filas
head(datos)

# Comprobamos que el objeto resultante es un data.frame
class(datos)

# Comprobamos la estructura de los datos
str(datos)

# Y realizamos un pequeño análisis estadístico
summary(datos)

# Seleccionamos las dos variables de interés
X <- datos[,"X"]
Y <- datos[,"Y47"]

# Guardamos el número de datos
n <- length(Y)
```

# 1) Relación entre variable explicativa y variable respuesta
En primer lugar, calculamos la covarianza y el coeficiente de correlación entre las variables:
$$
S_{xY}=\frac{1}{n}\sum_{i=1}^n (x_i-\overline{x})(Y_i-\overline{Y}))\quad\quad\quad 
r_{xY}=\frac{S_{xY]}}{\sqrt{S_x^2}\sqrt{S_Y^2}}
$$

Debemos tener en cuenta que R calcula la covarianza como una 'cuasi'covarianza, es decir, dividiendo entre $n-1$ en lugar de entre $n$. Para corregirlo, multiplicamos por $n-1$ y dividimos entre $n$, aunque también mostraremos el valor original. No afectará al coeficiente de correlación, pues el denominador se anula con los $\sqrt{n-1}$ de las cuasidesviaciones típicas.

```{r cov}
covar = cov(X,Y)*(n-1)/n; covar           # Covarianza
cov(X,Y)                                  # Cuasicovarianza
cor(X, Y)                                 # Coeficiente de correlación
```
Que la covarianza sea distinta de 0 nos indica que hay una relación lineal. Además, al ser negativa, deducimos que esta es indirecta/inversa, es decir, que al aumentar la variable X, la variable Y diminuye. 

Adionalmente, ya sabemos que la recta de regresión que obtendremos posteriormente será decreciente, pues la pendiente estimada, $\beta_1 = \frac{S_{xY}}{S_x^2}$, tiene el mismo signo que $S_{xY}$, al ser la varianza siempre no negativa, y hemos obtenido que $S_{xY} < 0$.

Por un razonamiento análogo, $r_{xY}$ también debe tener el mismo signo que $S_{xY}$ y, en efecto, esto es lo que observamos en los resultados. La interpretación de su signo es, por tanto, la misma que la expuesta para la covarianza (relación lineal inversa/indirecta).

Ahora bien, no podemos sacar conclusiones acerca de la magnitud de la covarianza, pues esta tiene unidades (que ni siquiera conocemos). Por el contrario, el coeficiente de correlación es adimensional y, de hecho, sabemos que $|r_{xY}| \in [-1,1]$. Como $|r_{xY}| > 0.75$, la relación entre las variables es fuerte, esto es, tienen una correlación significativa. Cuando representemos el diagrama de dispersión de los datos y sobre el mismo, la recta de regresión, observaremos que los puntos son próximos a esta.

### Representación gráfica
Para visualizar la relación entre la variable explicativa y la variable respuesta, emplearemos un diagrama de dispersión. 

En primer lugar, hallamos el vector de medias o centro de gravedad aplicando \texttt{mean} en ambas variables:

```{r medias}
mX <- mean(X)
mY <- mean(Y)

c(mX, mY) # Mostramos el vector de medias
```

Como diagrama básico, emplearemos la función \texttt{plot}. Como recurriremos a este gráfico en particular en varias ocasiones a lo largo de este documento, vamos a definir una función que englobe la representación:

```{r dispersion, echo = FALSE}
representar <- function(){
  plot(X, Y,
       main="Diagrama de dispersión", pch=16,
       sub="Relación entre la variable explicativa y la variable respuesta")
  
  # Añadimos un punto para el vector de medias
  points(mX, mY, pch=12, col=3, cex=2)
  
  # Añadimos dos rectas para dividir en cuadrantes, limitados por el vector de medias
  abline(v=mX, col=3, lty=1, lwd=2)   # Vertical
  abline(h=mY, col=3, lty=1, lwd=2)   # Horizontal
  
  # Añadimos una rejilla de fondo
  grid(lty = 2, col = "lightgray", lwd = 1)
}

representar() # Ejecutamos la función
```

Se puede ver que la nube de puntos toma una forma descendente, lo cual es coherente con la correlación negativa de X e Y. También vemos que los datos están, de forma aproximada, uniformemente alineados en torno a una forma rectilínea. Todo esto motiva el establecimiento de un modelo lineal para la relación entre ambas variables que, recordemos, son de la forma:
$$
Y = \beta_0 + \beta_1X + \epsilon
$$

Ajustamos entonces este modelo a nuestros datos mediante la función \texttt{lm}:
```{r modelo}
modelo = lm(Y~X); modelo
```
y obtenemos un intercepto $\beta_0 = 4.184$ y una pendiente de $\beta_1 = -1.023$, lo cual concuerda con lo observado anteriormente en la nube de puntos. 

En los siguientes ejercicios, analizaremos en profundidad diferentes caracteríssticas de este modelo y obtendremos inferencia a partir del mismo. Nótese que para que las conclusiones extraídas en estos ejercicios tengan validez, deberemos suponer que se cumplen las hipótesis de linealidad, homocedasticidad y normalidad e independencia de los errores. Las comprobaremos de forma precisa en el ejercicio 7, pero de no ser válida alguna de ellas, tendríamos que revisar y descartar multitud de resultados.

## Ejercicio 2
### Estimación puntual a mano
Para la estimación puntual de los parámetros intercepto $\beta_0$, pendiente
$beta_1$ y varianza del error $\sigma^2$ podemos aplicar directamente las fórmulas
obtenidas en la parte teórica de la asignatura:
```{r estpuntmano}
var.X <- var(X)*(n-1)/n
beta0.gorro = mY - covar*mX/var.X; beta0.gorro
beta1.gorro = covar/var.X; beta1.gorro
var.error = sum((Y - beta0.gorro - beta1.gorro*X)^2)/(n-2); var.error
sd.error = sqrt(var.error); sd.error
```

### Estimación puntual automática
De manera alternativa, podemos obtenerlas a partir del propio modelo creado anteriomente por $\mathbb{R}$:
```{r estpuntauto}
modelo
# Intercepto beta0gorro y pendiente beta1gorro
modelo$coefficients
# En modelo$residuals están los residuos
sum(modelo$residuals^2)/(n-2)
```


```{r dispersionReg, echo=FALSE}
representar()
abline(modelo, col="red", lwd=2)
```

Incluimos también una gráfica adicional usando la librería _ggplot2_ e incluyendo la región
o intervalo de confianza para los datos al nivel del 99%:
```{r dispersionRegGGPLOT, echo=FALSE}
p3 <- ggplot(datos, aes(x=X, y=Y)) +
  geom_point() +
  geom_smooth(formula=y~x, level=0.99, method=lm, color="red", fill="#666666", se=TRUE) + 
  labs(y = "Variable respuesta",
       x = "Variable explicativa",
       title = "Modelo lineal simple, con región de confianza al 95%")
p3
```

## Ejercicio 3
\textit{Calcula los intervalos de confianza para los parámetros del modelo de nivel 99%.
Interpreta los resultados obtenidos.}

A continuación, hallamos los intervalos para
El pivote es ((PASAR A LATEX)) beta0.gorro - beta0/(sqrt(var.gorro*(1/n+mX^2/(n*var.X))) que es una
T de Student con n-2 grados de libertad

```{r icbeta0mano}
beta0.cuantil <- qt(1-alfa/2, df=n-2); beta0.cuantil
beta0.extremoinferior <- beta0.gorro - beta0.cuantil * sqrt(var.error * (1/n + mX^2/(n*var.X)))
beta0.extremosuperior <- beta0.gorro + beta0.cuantil * sqrt(var.error * (1/n + mX^2/(n*var.X)))
beta0.IC <- c(beta0.extremoinferior, beta0.extremosuperior); beta0.IC
```

El pivote es ((PASAR A LATEX)) pivote beta1.gorro - beta1/sqrt(var.gorro/ (var.X * n)) que es una
T de Student con n-2 grados de libertad


```{r icbeta1mano}
beta1.cuantil <- beta0.cuantil
beta1.extremoinferior <- beta1.gorro - beta1.cuantil*sqrt(var.error/(var.X * n))
beta1.extremosuperior <- beta1.gorro + beta1.cuantil*sqrt(var.error/(var.X * n))
beta1.IC <- c(beta1.extremoinferior, beta1.extremosuperior); beta1.IC
```

El pivote para la varianza del error será ((PASAR A LATEX))
pivote -> (n-2)*var.error^2/varianzaerror^2 que es una chi-cuadrado con n-2
grados de libertad
```{r icvarsmano}
var.error.cuantilinferior <- qchisq(alfa/2, df=n-2)
var.error.cuantilsuperior <- qchisq(1-alfa/2, df=n-2)
var.error.extremoinferior <- (n-2)*var.error^2/var.error.cuantilsuperior
var.error.extremosuperior <- (n-2)*var.error^2/var.error.cuantilinferior
var.error.IC <- c(var.error.extremoinferior, var.error.extremosuperior); var.error.IC
```

Pero también podemos utilizar las funciones de R para hacerlo de forma automática:
- IC para beta0 y beta1 asumiendo que la varianza es desconocida
```{r icbeta0beta1}
confint(modelo, level=0.99)
```

No hay una automatización del cálculo de la varianza del error

## Ejercicio 4
\textit{Realiza los contrates de significación asociados al intercepto y a la pendiente del modelo de regresión considerado. Interpreta los resultados obtenidos. En base a los resultados obtenidos, ¿tendría sentido considerar otro modelo más sencillo?}
A continuación, realizaremos los contrastes de significación sobre los parámetros $\hat{\beta_0} y \hat{\beta_1}$ con el objetivo de determinar si este se podría simplificar a uno con menos variables o no. Las hipótesis de estos contrastes son, respectivamente:
$$
\begin{cases}
H_0: \beta_0 = 0\\
H_a: \beta_0 \neq 0
\end{cases}
$$
y
$$
\begin{cases}
H_0: \beta_1 = 0\\
H_a: \beta_1 \neq 0
\end{cases}
$$
En primer lugar, realizaremos los contrastes de forma manual. Esto se hará a partir de los estadísticos de contraste basados en el pivote de las estimaciones puntuales previas:
```{r signifmano}
# Contrastes de significación para beta0gorro y beta1gorro
beta0.t <- abs(beta0.gorro) / (sqrt(var.error * (1 / n + mX ^ 2 / (n * var.X)))); beta0.t
beta1.t <- abs(beta1.gorro) / (sd.error / sqrt(n * var.X)); beta1.t
# Ambos contrastes caen en la región de rechazo
beta0.t > beta0.cuantil 
beta1.t > beta1.cuantil
# El p-valor del 1º contraste es ~0 -> La hipótesis nula es falsa para cualquier nivel de signif. razonable
beta0.pvalor = dt(beta0.t, df=n-2); beta0.pvalor

# El p-valor del 2º contraste es ~0 -> La hipótesis nula es falsa para cualquier nivel de signif. razonable 
beta1.pvalor = dt(beta1.t, df=n-2); beta1.pvalor
```

De esto deducimos que existen pruebas estadísticamente significativas de que $\beta_0 \neq 0$, lo cuál nos indica que el intercepto es distinto de 0. Por otro lado, también existen pruebas de $\beta_1 \neq 0$, de dónde deducimos que realmente la variable explicativa influye en la variable respuesta. 

Alternativamente, podemos obtener los valores de estos dos contrastes de significación y su p-valor a partir de los datos presentes en el modelo de R. Para esto, usaremos la función summary:
```{r signifauto}
summary(modelo)
```
En concreto, los valores relevantes son el t-value y el Pr(>|t|) de las filas (Intercept) y X que se corresponden al valor observado del estadístico observado y su p-valor en el contraste sobre el intercepto $\beta_0$ y la pendiente $\beta_1$. Obtenemos los mismos datos que en el cálculo manual.

En base a los resultados obtenidos anteriormente, decidimos no simplificar más nuestro modelo y continuar realizando regresión lineal.

## Ejercicio 5
\textit{Si consideramos que la variable X toma 3 nuevos valores: 2, 4 y 6 unidades, proporciona intervalos de predicción e intervalos de confianza para la media condicionada de la variable Y. Interpreta los resultados obtenidos.}

En este apartado, consideramos 3 nuevos valores para la variable explicativa $X = 2, 4, 6$. Para obtener intervalos de confianza para la media de Y condicionada a estos valores y de predicción, es necesario comprobar primero que estos datos están dentro del rango de observación de X. Esto es debido a que no sabemos como se comporta el modelo fuera del rango observado, y nuestro objetivo es predecir y no extrapolar.
```{r validacionnuevosdatos}
nuevosValores <- c(2, 4, 6)
nuevosValores
# Los nuevos valores están contenidos dentro del rango observado
min(X) < min(nuevosValores) && max(X) > max(nuevosValores)
# Construimos un data.frame  con los nuevos datos ya que predict necesita este formato para sus predicciones
nuevosDatos = data.frame("X" = nuevosValores)
```
Habiendo realizado esta comprobación, ya podemos obtener los intervalos utilizando la función predict sobre el modelo de R. Obtendremos ambos intervalos para los niveles de significación fijado anteriormente $\alfa = 0.99$.

En primer lugar, pasando el argumento $\texttt{interval = "confidence"}$ obtenemos los asociados a la media condicionada.
```{r intmediacond}
predict(modelo, newdata = nuevosDatos, interval = "confidence", level = alfa)
```
Y para obtener los intervalos de predicción, los cuáles serán más amplios que los anteriores, pasamos el argumento $\texttt{interval = "prediction"}$
```{r intprediccion}
predict(modelo, newdata = nuevosDatos, interval = "prediction", level = alfa)
```
## Ejercicio 6 
\textit{Calcula alguna medida de bondad de ajuste del modelo lineal simple considerado.}
El objetivo del cálculo de una medida de bondad de ajuste es determinar como de "bueno" o "potente" es un modelo de regresión. En terminos estadísticos, esto se traduce a cuánta proporción de la variabilidad de $Y$ puede ser explicada por el modelo de regresión. En el caso del modelo lineal, podemos descomponer la varianza como
$$
\sigma_Y^2 = 
\sum_{i=1}^n (Y_i - \overline{Y})^2 = 
\sum_{i=1}^n (Y_i - \hat{\beta_0} - \hat{\beta_1}*x_i)^2 + 
\sum_{i=1}^n (\hat{\beta_0} + \hat{\beta_1}*x_i - \overline{Y})^2
$$

Dónde observamos que el segundo sumando representa la varianza explicada por la recta de regresión, mientras que el primero representa la no explicada. Es por tanto necesario que el cociente $\frac{RSS}{TSS}$, dónde TSS (\textit{Total Sum of Squares}) es la varianza total de $Y$ y RSS (\textit{Residual Sum of Squares}) es la no explicada por el modelo, sea lo menor posible.
De esta forma, definimos el coeficiente de determinación $ R^2 = 1 - \frac{RSS}{TSS}$. En primer lugar, calcularemos este cociente a mano:

```{r coefdeterman}
rss = sum((Y - beta0.gorro - beta1.gorro*X)^2); rss
tss = var(Y)
r2 = 1 - rss/tss; r2
```
Aunque también podemos obtenerla a partir del modelo de R, en el campo \textit{Adjusted R-Squared}:
```{r coefdeterauto}
summary(modelo)
```
Obtenemos un valor de 0.903, lo cuál nos indica que nuestro modelo es bastante potente en términos habituales. Cabe destacar que el hecho de que el coeficiente de determinación sea alto, no nos indica aún que nuestro modelo sea correcto, lo cuál será discutido en el apartado 7. El recíproco tampoco es cierto, podríamos tener un modelo correcto que no fuera bueno.

## Ejercicio 7
  Las técnicas de inferencia empleadas hasta el momento son ciertas bajo el supuesto de que las 4 hipótesis del modelo de regresión lineal simple (linealidad, homocedasticidad, normalidad e independencia) se verifican. De lo contrario,
  no todas las interpretaciones obtenidas seguirían siendo válidas. Por ejemplo, si no se cumplieran las hipótesis de homocedasticidad, normalidad e independencia, los intervalos de confianza que hemos obtenido no serían válidos.

### Linealidad
En primer lugar, podemos tratar de aventurar si se los datos siguen una tendencia lineal. Emplearemos una aproximación exploratoria, a través de una interpretación gráfica. Para ello, revisitemos la representación previamente definida.

```{r dispersion2, echo=FALSE}
representar()
abline(modelo, col="red", lwd=2)
```

Vemos que los puntos parecen distribuirse en torno a la recta de forma lineal. Si bien hay datos un tanto atípicos, especialmente en los extremos, esto no es lo suficientemente significativo como para rechazar la hipótesis. Tampoco se ve un patrón evidente en los datos (es esto lo que debemos tratar de detectar, y no solo corroborar que haya el mismo número de puntos por encima/debajo de la recta, que no es suficiente como para indicar linealidad).

Nótese que aunque se puede apreciar una menor concentración de puntos para valores de X comprendidos alrededor del valor 4, esto no es indicativo de una falta de linealidad. Dado que trabajamos bajo diseño fijo, se tiene que achacar a decisiones sobre las condiciones de medición o al propio diseño del experimento. Esta observación se puede comprobar a través del siguiente cuadro:

```{r tabla}
# Representamos el número de valores de X en cada intervalo de longitud 0.5, comenzando desde el mayor entero menor
# o igual que el dato mínimo, y finalizando en el menor entero mayor o igual que el dato máximo.
table(cut(X, breaks=seq(from=floor(min(X)), to=ceiling(max(X)), by=0.5)))
```

Con el objetivo de realizar una prueba más precisa, planteamos el siguiente contraste de hipótesis. Como hipótesis nula tenemos que la variable respuesta siga el modelo lineal simple que hemos estado considerando, y como hipótesis nula, que siga un modelo parabólico, donde hay dependencia de la variable explicativa al cuadrado:
$$
\begin{cases}
H_0: Y=\beta_0+\beta_1X+\epsilon\\
H_a: Y=\beta_0+\beta_1X+\beta_2*X^2\epsilon
\end{cases}
$$

Ejecutamos la prueba:

```{r resettest}
# Empleamos power = 2 porque estamos considerando una alternativa cuadrática
resettest(modelo, power = 2)
```

Vemos que el p-valor es de 0.7613. INTERPRETAR.

No obstante, este contraste solo nos ha aportado información sobre la equiparación con un modelo cuadrático. Si buscáramos una confirmación perfecta, teórica, deberíamos seguir contrastando con todos los valores de power. Dado que esto es impracticable experimentalmente, podemos plantearnos en su lugar un contraste más general, con una alternativa no parámetrica:
$$
\begin{cases}
H_0: Y=\beta_0+\beta_1X+\epsilon\\
H_a: Y=m(X)+\epsilon
\end{cases}
$$

Haciendo uso del paquete _sm_, realizamos la prueba de hipótesis:
```{r smregresion}
         
# Importamos rpanel para abrir un panel interactivo para la representación
# Los valores que sabemos interpretar son los que aparecen con las opciones por defecto
# Indicamos test=T para que se nos muestre un p-valor.
sm.regression(X, Y, model="linear", panel=T, test=T)
```

La interpretación de la figura resultante es la siguiente. Con una línea negra nos aparece marcada una estimación no paramétrica de la regresión (sin asumir linealidad), y en azul, una región de confianza para el modelo lineal simple. Vemos que la línea negra se encuentra siempre dentro de la región azul. Por tanto, podemos asumir que la hipótesis nula es cierta, esto es, que los datos verifican la hipótesis de linealidad.
FALTA ANALIZAR EL P-VALOR


### Homocedasticidad
Contrapongamos ahora los residuos del modelo a la variable explicativa. Se muestra también el diagrama de dispersión original:
```{r dispersionresiduos, echo=FALSE}
par(mfrow=c(1,2))

representar()
abline(modelo, col="red", lwd=2)

residuos <- modelo$residuals

plot(X, residuos,
   main="Diagrama de dispersión", pch=16,
   sub="Subtítulo")
abline(h=0, col="gray", lwd=2)

par(mfrow=c(1,1))

```

Queremos comprobar ahora si la varianza del error, $\sigma^2$, es la misma independientemente del valor que tome la variable explicativa. Vemos que la distribución de los residuos en el diagrama no sigue un patrón evidente, y que su desviación con respecto a la recta $x=0$ parece ser la misma sin importar el intervalo de X considerado. 

Tampoco sobre el diagrama de dispersión de la variable respuesta observamos una tendencia significativa acera de las desviaciones con la recta de regresión. En conjunción con lo anterior, podríamos aventurar, a primera vista, que los datos muestrales son verdaderamente homocedásticos.

Sí destacamos que la interpretación para la región central, en aproximadamente $(4, 4.5)$, puede no ser muy precisa, por falta de datos. Sin embargo, esto no basta para desmentir la hipótesis de homocedasticidad.

Para tener una confirmación precisa, nos planteamos el siguiente contraste de hipótesis:
$$
\begin{cases}
H_0: \text{modelo homocedástico}\\
H_a: \text{modelo heterocedástico}
\end{cases}
$$

Ejecutamos un test de Harrison-McCabe con R, haciendo uso del previamente cargado paquete _lmtest_:
```{r hmctest}
hmctest(Y~X)
```
El p-valor es de 0.763.


### Normalidad
Para corroborar que el error tiene distribución normal, haremos varias representaciones gráficas que nos permitan intuir si la hipótesis se ajusta a los datos. Trabajaremos con los residuos estandarizados, pues no tienen la misma varianza y la correlación entre cada 2 de ellos puede ser distinta (provienen de distribuciones diferentes).

Presentamos 3 gráficos: un histograma, un boxplot y un qqplot (para el cual necesitamos la librería _car_), aunque centraremos nuestra atención en el último de ellos, el más relevante en lo que concierne al estudio de la normalidad.

```{r representacionnormalidad, echo=FALSE}
residuos.estan = rstandard(modelo)

par(mfrow = c(1, 3 )) 

hist(residuos.estan, col=c(viridis(n=5, begin=0, end=0.8), viridis(n=1, begin=1), viridis(n=5, begin=0.8, end=0)))
rug(residuos.estan)

boxplot(residuos.estan, col="gray")

qqPlot(residuos.estan)
```

En el histograma podemos apreciar una cierta asimetría hacia la derecha (valores más altos). En el boxplot o diagrama de caja vemos que la media está centrada en el centro de la caja, un buen indicador. No obstante, la cola izquierda es de una longitud ligeramente mayor, lo cual es indicativo de la asimetría mencionada, al estar los datos más concentrados alrededor de valores más altos.

El QQPlot o diagrama cuantil-cuantil nos presenta una comparativa entre los cuantil muestrales de los residuos estandarizados y los cuantiles teóricos de una normal estándar. Si los residuos estandarizados presentaran una distribución normal de media 0 y varianza 1, se situarían alrededor de la recta diagonal resaltada. En nuestro caso, vemos que en la zona central el ajuste es bueno, pero hay una cierta desviación en las colas. Esto es especialmente notorio en la superior, donde los cuantiles muestrales son algo inferiores a los cuantiles teóricos de una normal, que es lógico y coherente con la asimetría indicada anteriormente.

Ahora bien, una representación visual es solamente un apoyo al estudio, y no podemos inferir de ella una conclusión estadísticamente definitoria. Para ello , emplearemos  directamente un test de bondad de ajuste sobre los errores estandarizados con respecto a una distribución normal. Aunque hay varias opciones adecuadas, como el test de Kolmogorov-Smirnov y el test de Lilliefoids, el más ampliamente usado con este propósito es el test de Shapiro-Wilk, especialmente diseñado para contrastes de normalidad:

$$
\begin{cases}
H_0: \epsilon\text{ sigue una distribución normal}\\
H_a: \epsilon\text{ no sigue una distribución normal}
\end{cases}
$$

Ejecutemos pues el contraste de especificación mencionado:
```{r shaphiro}
shapiro.test(residuos.estan)
```

También podemos comprobar los resultados de otros tests:
```{r kslillie}
# TODO FIXME
# el argumento y esta ausente
# ks.test(residuos.estan)
lillie.test(residuos.estan)
```

Una observación adicional: en este caso, tenemos que el tamaño de la muestra, n, es mayor que 30, de modo que se pueden despreciar las impurezas debidas a utilizar los residuos en el estudio de la normalidad, en lugar de los errores (que no están sujetos a la aplicación del ajuste de mínimos cuadrados).
```{r inputsize}
n
```


### Independencia
De entre las 4 hipótesis con las que trabaja el modelo, la independencia de los errores es la más difícil de corroborar. No tenemos información acerca del proceso de recogida de muestras, por lo que no podemos garantizarla en base a que los datos hayan sido medidos sobre objetos o individuos de forma independiente.

Debido a la complejidad inherente a este apartado, nos limitaremos a comprobar la independencia temporal. Para ello, asumiremos que nuestros datos han sido medidos a lo largo del tiempo. 

Nos preguntamos entonces si existe algún tipo de relación entre las observaciones, esto es:
$$
\begin{cases}
H_0: \epsilon\text{ son incorrelacionados}\\
H_a: \epsilon\text{ son correlacionados de orden k}
\end{cases}
$$
En el contraste planteado, $k\in\mathbb{N}$, $k>1$, es el retardo, esto es, la separación entre los instantes de tiempo que influyen sobre el instante actual. Así, fijado un k y dados unos errores 


BONDAD DE AJUSTE ES EL COEFICIENTE DE DETERMINACIÓN (ES UNA MEDIDA DE CUÁNTO DE BUENO ES EL MODELO). EL R^2 AJUSTADO ES OTRA MEDIDA DE BONDAD DE AJUSTE. TAMBIÉN HAY CONTRASTES. ESTÁ EN EL SUMMARY.

ACEPTAR LA H0 EN CONTRASTE LINEALIDAD POLINÓMICO SOLO SIGNIFICA QUE MI MODELO ES MEJOR QUE UN POLINÓMICO DE ORDEN 2, 3...

EL SM TE LO CONTRASTA CON ALTERNATIVA NO PARAMÉTRICA -> ES MEJOR MI MODELO QUE CUALQUIER OTRA COSA? CON HACER ESTE ES SUFICIENTE

a veces las formas raras en homocedasticidad (qqplot) se pueden deber a falta de linealidad. Es más conncluyente el contraste


HAY QUE FIJAR EL ALFA DESDE EL PRINCIPIO. METER YA DESDE LA INTRODUCCIÓN. EN ESTE CASO TENDREMOS QUE FIJAR ALFA = 1% (NIVEL 99%) POR EL APARTADO 3

