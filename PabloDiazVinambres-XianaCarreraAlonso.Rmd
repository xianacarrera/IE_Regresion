---
title: "Trabajo de Evaluación Continua de Modelos de Regresión"
subtitle: Curso 2021/2022
author: "Xiana Carrera Alonso, Pablo Díaz Viñambres"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Poner introducción aquí


Además, en este tipo de documentos también puedes emplear sintaxis de LaTeX como ecuaciones:

$$
\begin{cases}
H_0: \beta_1=0,\\
H_a: \beta_1\neq 0
\end{cases}
$$
o tablas:
\begin{center}
\begin{tabular}{ |c c c| }
 \hline
 celda1 & celda2 & celda3 \\
 \hline
 celda4 & celda5 & celda6 \\  
 celda7 & celda8 & celda9 \\   
  \hline
\end{tabular}
\end{center}


## Incluir gráficos

Por otra parte, también puedes incluir representaciones gráficas en tu documento empleando la sintaxis:

```{r pressure, echo=FALSE}
boxplot(cars$dist)
```

Fíjate que añadiendo el parámetro `echo = FALSE' en las opciones del \emph{chuck} evitamos que se imprima el código de R que empleamos para generar la representación gráfica.


En primer lugar, leemos los datos del archivo y los colocamos en las variables X e Y.
Asimismo, leemos el número de datos n.

```{r cars}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # Configurar wd a la carpeta actual (solo en RStudio) 
#setwd("C:\\Users\\Pablo\\Desktop\\IE_Regresion")
datos <- read.table("datos_trabajo_temas6y7.txt", header=T, sep=" ", dec=".")
str(datos)
summary(datos)
head(datos)


datos = datos[,c("X", "Y47")]
str(datos)
X <- datos[,"X"]
Y <- datos[,"Y47"]

n <- length(Y)
```

## Ejercicio 2
En primer lugar, calculamos la covarianza y el coeficiente de correlación de los datos,
con el objetivo de ver si existe relación lineal entre las variables.

```{r cars}
covar = cov(X,Y)*(n-1)/n; covar           # Covarianza
cov(X,Y)                                  # Cuasicovarianza
cor(X, Y)                                 # Coeficiente de correlación
```

A  continuación hallamos el vector de medias o centro de gravedad 
para añadirlo posteriormente a la gráfica:

```{r cars}
mX <- mean(X)
mY <- mean(Y)
```

Y con la siguiente función generamos la gráfica de dispersión básica:

```{r pressure, echo = FALSE}
representar <- function(){
  plot(X, Y,
       main="Diagrama de dispersión", pch=16,
       sub="Subtítulo")
  
  # Añadimos un punto para el vector de medias
  points(mX, mY, pch=12, col=3, cex=2)
  
  # Añadimos dos rectas para dividir en cuadrantes
  abline(v=mX, col=3, lty=1, lwd=2)   # Vertical
  abline(h=mY, col=3, lty=1, lwd=2)   # Horizontal
  
  
  grid(nx = NULL, ny = NULL, lty = 2, col = "lightgray", lwd = 1)
}

representar()
```

Finalmente, establecemos el modelo lineal entre los datos 
```{r cars}
modelo=lm(Y~X)
modelo
```

## Ejercicio 2
### Estimación puntual a mano
Para la estimación puntual de los parámetros intercepto $\beta_0$, pendiente
$beta_1$ y varianza del error $\sigma^2$ podemos aplicar directamente las fórmulas
obtenidas en la parte teórica de la asignatura:
```{r cars}
var.X <- var(X)*(n-1)/n
beta0.gorro = mY - covar*mX/var.X; beta0.gorro
beta1.gorro = covar/var.X; beta1.gorro
var.error = sum((Y - beta0.gorro - beta1.gorro*X)^2)/(n-2); var.error
sd.error = sqrt(var.error); sd.error
```

### Estimación puntual automática
De manera alternativa, podemos obtenrlas a partir del propio modelo creado anteriomente por $\mathbb{R}$:
```{r cars}
modelo    # Información del modelo
modelo$coefficients         # beta0 gorro y beta1 gorro

# En modelo$residuals están los residuos
sum(modelo$residuals^2)/(n-2)
```


```{r pressure, echo=FALSE}
representar()
abline(modelo, col="red", lwd=2)
```

Incluimos también una gráfica adicional usando la librería _ggplot2_ e incluyendo la región
o intervalo de confianza para los datos al nivel del 99%:
```{r pressure, echo=FALSE}
library(ggplot2)
p3 <- ggplot(datos, aes(x=X, y=Y)) +
  geom_point() +
  geom_smooth(formula=y~x, level=0.99, method=lm, color="red", fill="#666666", se=TRUE) + 
  labs(y = "Variable respuesta",
       x = "Variable explicativa",
       title = "Modelo lineal simple, con región de confianza al 95%")
p3
```

## Ejercicio 3
### A mano
En el siguiente apartado, En primer lugar, establecemos el nivel de significación $\alfa$.
```{r cars}
alfa <- 1 - 0.99
```

A continuación, hallamos los intervalos para 
El pivote es ((PASAR A LATEX)) beta0.gorro - beta0/(sqrt(var.gorro*(1/n+mX^2/(n*var.X))) que es una
T de Student con n-2 grados de libertad

```{r cars}
beta0.cuantil <- qt(1-alfa/2, df=n-2); beta0.cuantil
beta0.extremoinferior <- beta0.gorro - beta0.cuantil * sqrt(var.error * (1/n + mX^2/(n*var.X)))
beta0.extremosuperior <- beta0.gorro + beta0.cuantil * sqrt(var.error * (1/n + mX^2/(n*var.X)))
beta0.IC <- c(beta0.extremoinferior, beta0.extremosuperior); beta0.IC
```

El pivote es ((PASAR A LATEX)) pivote beta1.gorro - beta1/sqrt(var.gorro/ (var.X * n)) que es una
T de Student con n-2 grados de libertad

```{r cars}
beta1.cuantil <- beta0.cuantil
beta1.extremoinferior <- beta1.gorro - beta1.cuantil*sqrt(var.error/(var.X * n))
beta1.extremosuperior <- beta1.gorro + beta1.cuantil*sqrt(var.error/(var.X * n))
beta1.IC <- c(beta1.extremoinferior, beta1.extremosuperior); beta1.IC
```

El pivote para la varianza del error será ((PASAR A LATEX))
pivote -> (n-2)*var.error^2/varianzaerror^2 que es una chi-cuadrado con n-2
grados de libertad
```{r cars}
var.error.cuantilinferior <- qchisq(alfa/2, df=n-2)
var.error.cuantilsuperior <- qchisq(1-alfa/2, df=n-2)
var.error.extremoinferior <- (n-2)*var.error^2/var.error.cuantilsuperior
var.error.extremosuperior <- (n-2)*var.error^2/var.error.cuantilinferior
var.error.IC <- c(var.error.extremoinferior, var.error.extremosuperior); var.error.IC
```

Pero también podemos utilizar las funciones de R para hacerlo de forma automática:
- IC para beta0 y beta1 asumiendo que la varianza es desconocida
```{r cars}
confint(modelo, level=0.99)
```

No hay una automatización del cálculo de la varianza del error

# Ejercicio 4
A continuación, realizaremos los contrastes de significación sobre el modelo con el objetivo de determinar si el modelo se podría simplificar a uno con menos variables o no.
En primer lugar, realizaremos el contraste de forma manual a partir de los estadísticos de contraste basado en el pivote de la estimaciones puntuales previas:
```{r cars}
#contraste de significacion para beta0
beta0.t <- abs(beta0.gorro) / (sqrt(var.error * (1/n + mX^2/(n*var.X)))); beta0.t
beta1.t <- abs(beta1.gorro) / (sd.error / sqrt(n*var.X)); beta1.t
beta0.t > beta0.cuantil # TRUE --> Aceptamos la hipótesis nula de que el modelo tiene origen distinto de 0 
beta1.t > beta1.cuantil # TRUE --> Aceptamos la hipótesis nula de que el modelo depende de la variable X
dt(beta0.t, df=n-2); # El p-valor es 0 --> La hipótesis nula es cierta para cualquier nivel de signif.

```

```
```{r cars}
summary(modelo)
```

#*******************************************************************************
# Ejercicio 5
#*******************************************************************************

x0 <- c(2, 4, 6)

y0.tilde <- beta0.gorro + beta1.gorro * x0


#*********** IC







# Con qué nivel de confianza?
predict(modelo, newdata=data.frame("X"=x0), interval = "confidence", level=0.99)
predict(modelo, newdata=data.frame("X"=x0, interval = "prediction"), level=0.99)





# Ejercicio 7
  Las técnicas de inferencia empleadas hasta el momento son ciertas bajo el supuesto de que las 4 hipótesis del modelo de regresión lineal simple (linealidad, homocedasticidad, normalidad e independencia) se verifican. De lo contrario,
  no todas las interpretaciones obtenidas seguirían siendo válidas. Por ejemplo, si no se cumplieran las hipótesis de homocedasticidad, normalidad e independencia, los intervalos de confianza que hemos obtenido no serían válidos.

## Linealidad
En primer lugar, podemos tratar de aventurar si se los datos siguen una tendencia lineal. Emplearemos una aproximación exploratoria, a través de una interpretación gráfica. Para ello, revisitemos la representación previamente definida.

```{r pressure, echo=FALSE}
representar()
abline(modelo, col="red", lwd=2)
```

Vemos que los puntos parecen distribuirse en torno a la recta de forma lineal. Si bien hay datos un tanto atípicos, especialmente en los extremos, esto no es lo suficientemente significativo como para rechazar la hipótesis. Tampoco se ve un patrón evidente en los datos (es esto lo que debemos tratar de detectar, y no solo corroborar que haya el mismo número de puntos por encima/debajo de la recta, que no es suficiente como para indicar linealidad).

Nótese que aunque se puede apreciar una menor concentración de puntos para valores de X comprendidos alrededor del valor 4, esto no es indicativo de una falta de linealidad. Dado que trabajamos bajo diseño fijo, se tiene que achacar a decisiones sobre las condiciones de medición o al propio diseño del experimento. Esta observación se puede comprobar a través del siguiente cuadro:

```{r cars}
# Representamos el número de valores de X en cada intervalo de longitud 0.5, comenzando desde el mayor entero menor
# o igual que el dato mínimo, y finalizando en el menor entero mayor o igual que el dato máximo.
table(cut(X, breaks=seq(from=floor(min(X)), to=ceiling(max(X)), by=0.5)))
```

Con el objetivo de realizar una prueba más precisa, planteamos el siguiente contraste de hipótesis. Como hipótesis nula tenemos que la variable respuesta siga el modelo lineal simple que hemos estado considerando, y como hipótesis nula, que siga un modelo parabólico, donde hay dependencia de la variable explicativa al cuadrado:
$$
\begin{cases}
H_0: Y=\beta_0+\beta_1X+\epsilon\\
H_a: Y=\beta_0+\beta_1X+\beta_2*X^2\epsilon
\end{cases}
$$

Ejecutamos la prueba:

```{r cars}
library(lmtest)
# Empleamos power = 2 porque estamos considerando una alternativa cuadrática
resettest(modelo, power = 2)
```

Vemos que el p-valor es de 0.7613. INTERPRETAR.

No obstante, este contraste solo nos ha aportado información sobre la equiparación con un modelo cuadrático. Si buscáramos una confirmación perfecta, teórica, deberíamos seguir contrastando con todos los valores de power. Dado que esto es impracticable experimentalmente, podemos plantearnos en su lugar un contraste más general, con una alternativa no parámetrica:
$$
\begin{cases}
H_0: Y=\beta_0+\beta_1X+\epsilon\\
H_a: Y=m(X)+\epsilon
\end{cases}
$$

Haciendo uso del paquete _sm_, realizamos la prueba de hipótesis:
```{r cars}
library(sm)
library(rpanel)            
# Importamos rpanel para abrir un panel interactivo para la representación
# Los valores que sabemos interpretar son los que aparecen con las opciones por defecto
# Indicamos test=T para que se nos muestre un p-valor.
sm.regression(X, Y, model="linear", panel=T, test=T)
```

La interpretación de la figura resultante es la siguiente. Con una línea negra nos aparece marcada una estimación no paramétrica de la regresión (sin asumir linealidad), y en azul, una región de confianza para el modelo lineal simple. Vemos que la línea negra se encuentra siempre dentro de la región azul. Por tanto, podemos asumir que la hipótesis nula es cierta, esto es, que los datos verifican la hipótesis de linealidad.
FALTA ANALIZAR EL P-VALOR


## Homocedasticidad
Contrapongamos ahora los residuos del modelo a la variable explicativa. Se muestra también el diagrama de dispersión original:
```{r cars}
par(mfrow=c(1,2))

representar()
abline(modelo, col="red", lwd=2)

residuos <- modelo$residuals

plot(X, residuos,
   main="Diagrama de dispersión", pch=16,
   sub="Subtítulo")
abline(h=0, col="gray", lwd=2)

par(mfrow=c(1,1))

```

Queremos comprobar ahora si la varianza del error, $\sigma^2$, es la misma independientemente del valor que tome la variable explicativa. Vemos que la distribución de los residuos en el diagrama no sigue un patrón evidente, y que su desviación con respecto a la recta $x=0$ parece ser la misma sin importar el intervalo de X considerado. 

Tampoco sobre el diagrama de dispersión de la variable respuesta observamos una tendencia significativa acera de las desviaciones con la recta de regresión. En conjunción con lo anterior, podríamos aventurar, a primera vista, que los datos muestrales son verdaderamente homocedásticos.

Sí destacamos que la interpretación para la región central, en aproximadamente $(4, 4.5)$, puede no ser muy precisa, por falta de datos. Sin embargo, esto no basta para desmentir la hipótesis de homocedasticidad.

Para tener una confirmación precisa, nos planteamos el siguiente contraste de hipótesis:
$$
\begin{cases}
H_0: \text{modelo homocedástico}\\
H_a: \text{modelo heterocedástico}
\end{cases}
$$

Ejecutamos un test de Harrison-McCabe con R, haciendo uso del previamente cargado paquete _lmtest_:
```{r cars}
hmctest(Y~X)
```
El p-valor es de 0.763.


## Normalidad
Para corroborar que el error tiene distribución normal, haremos varias representaciones gráficas que nos permitan intuir si la hipótesis se ajusta a los datos. Trabajaremos con los residuos estandarizados, para tener en cuenta que la correlación entre 2 de ellos puede variar.

Presentamos 3 gráficos: un histograma, un boxplot y un qqplot (para el cual necesitamos la librería _car_), aunque centraremos nuestra atención en el último de ellos, el más relevante en lo que concierne al estudio de la normalidad.

```{r cars}
residuos.estan = rstandard(modelo)

par(mfrow = c(1, 3 )) 

library(viridis)
hist(residuos.estan, col=viridis(11))
rug(residuos.estan)

boxplot(residuos.estan, col="gray")

library(car)
qqPlot(residuos.estan)
```



emplearemos directamente un test de bondad de ajuste sobre los errores estandarizados, con respecto a dicha distribución. Aunque hay varias opciones adecuadas, como el test de Kolmogorov-Smirnov y el test de Lilliefoids, el más ampliamente usado con este propósito es el test de Shapiro-Wilk, especialmente diseñado para contrastes de normalidad:

$$
\begin{cases}
H_0: \epsilon\text{ sigue una distribución normal}\\
H_a: \epsilon\text{ no sigue una distribución normal}
\end{cases}
$$

Como observación, comentamos que el motivo por el cual se estandarizan los residuos es para tener en cuenta que la correlación entre 2 de ellos puede variar.

Ejecutemos pues el contraste de especificación mencionado:
```{r cars}

```



